\section{Angriffsmöglichkeiten auf Neuronale Netzwerke} \label{chpt:Stand_der_Technik_Angriffe}
Neuronale Netzwerke, obwohl leistungsfähig und vielseitig einsetzbar, eröffnen verschiedene Möglichkeiten für Angriffsvektoren, die von potenziellen Angreifern ausgenutzt werden können, um erheblichen Schaden anzurichten. Die Angriffe reichen von sogfältig konzipierten Manipulationen des Trainisngsprozesses bis hin zu raffinierten Methoden oder Modellinferenzen. Das ganze bringt  Konsequenzen für Entwickler und Unternehmen von Beeinträchtigung der KI-System-Verfügbarkeit bis hin zu gravierenden Datenschutzverletzungen von Involvierten mit sich.

Im Bereich der Bildklassifizierung hat sich ein gängiges Angriffsmuster in Form der sogenannten \glqq Adversarial Attacks\grqq{} etabliert. Bei dieser Art von Angriff zielt der Angreifer darauf ab, die Klassifizierungsergebnisse eines Modells zu manipulieren, indem er gezielt Störungen in den Eingabedaten einführt. Dies kann beispielsweise durch die Anwendung spezifischer Filter geschehen, die das Modell dazu verleiten, fehlerhafte Vorhersagen zu treffen. Diese Störungen sind häufig für das menschliche Auge kaum wahrnehmbar, führen jedoch zu erheblichen Beeinträchtigungen der Modellleistung bei der Verarbeitung von unvorhergesehenen, mit Störungen versehenen Daten. Adversarial Attacks gehören zu einer Gruppe von Angriffen, die darauf abzielen, die Genauigkeit von Modellen durch die Anwendung unterschiedlicher Techniken zu beeinträchtigen.

Zusätzlich zu Angriffen, die direkt auf die Modellvorhersagen abzielen, existieren auch solche, die die Qualität des Trainingsprozesses und die Gesamtleistung der Modelle beeinflussen. Ein Beispiel hierfür sind "Poisoning-Angriffe". Bei dieser Angriffsmethode werden absichtlich manipulierte Datenpunkte in Trainingsdatensätze eingeschleust, um die Qualität des Modelltrainings zu beeinträchtigen. Durch gezieltes Hinzufügen von verfälschten und missbrauchten Datenpunkten kann die Generalisierungsfähigkeit des Modells gegenüber neuen Daten erheblich beeinträchtigt werden.

Des Weiteren existieren diverse Angriffsmethoden, die darauf abzielen, die Modellintegrität und -sicherheit zu untergraben. Hierzu gehören "Transfer Attacks", bei denen Adversarial-Störungen von einem Modell auf ein anderes übertragen werden, um die Angriffseffektivität zu maximieren.

Eine weitere Angriffskategorie beschreiben die sogenannten Inferenz-Angriffe. Diese fokussieren sich auf den Versuch der Extraktion von Daten- und Modellparametern aus bereits trainierten neuronalen Netzwerken. Der Angreifer versucht durch gezielte Schwachstellenausnutzung und Analyse des Netzwerks beispielsweise herauszufinden, ob ein bestimmter Datenpunkt Teil des Trainingsdatensatzes war. Ein solcher Angriff wird als \glqq Membership-Inference-Angriff\grqq{} bezeichnet und wird in der Regel in Black-Box-Szenarien durchgeführt, in denen Modellparameter und genutzte Datensätze nicht bekannt sind. 

Die im Folgenden beschriebene Angriffsart der \glqq Modell-Inversionsangriffe\grqq{} gehört zu den Inferenz-Angriffen und bezeichnet eine Vorgehensweise, bei der Angreifer mittels gezielter Analyse von Modellausgaben im Rahmen unterschiedlicher Verfahren den Versuch unternehmen, Trainingsdaten zu rekonstruieren.

% Data Poisoning (Trainingsprozess)
% Inferenz Angriffe (Model Inverison, Model Extraction, Membership inferenz)
% Adversarial Attacks
\subsection{Modell-Inversionsangriffe}
Bei Modell-Inversionsangriffen wird die Interaktion mit einem trainierten neuronalen Netzwerk genutzt, um Einblicke in den zugrunde liegenden Trainingsdatensatz zu gewinnen. Dies geschieht durch eine systematische Analyse der Ausgaben des Modells in Verbindung mit spezifischen Inversionsalgorithmen, die darauf abzielen, die Eingabedaten, die zu den beobachteten Modellausgaben geführt haben, zu rekonstruieren. Der Angreifer strebt dabei an, latente Merkmale und Muster der Trainingsdaten zu extrahieren, was nicht nur eine potenzielle Verletzung der Privatsphäre der in den Trainingsdatensatz eingeflossenen Informationen darstellen kann, sondern auch die Gefahr birgt, dass sensible oder persönliche Daten durch die Analyse des Modells exponiert werden.
\subsubsection{Angriffsziel}
Das Hauptziel von Modell-Inversionsangriffen besteht darin, interene Merkmale, Strukturen und Muster der im Training verwendeten Daten zu extrahieren, um eine mögliche Rekonstruktion auszuführen. Dabei geht man, gegensätzlich zu \glqq Adversarial Attacks\grqq{}, den Weg vom Ausgabebereich des Modells hin zum Eingabebereich, um damit Generierungen der Daten auszuführen. Dies geschieht auf Basis der gezielten Analyse der Ausgabe eines Modells bezüglich eines Inputs $I$. Neben den sichtlichen Merkmalen zielt diese Angriffsart auch auf die Rekonstruktion latenter Merkmale und Muster ab. Daneben kann man durch die Durchführung Informationen über die Art der Daten und verschiedene Muster im Training erlangen, um darauf auf die Reaktion bezüglich bestimmter Datenpunkte schließen zu können.

Durch die Extraktion latenter und visueller Merkmale der Daten kann bei Verwendung sensibler Daten eine Verletzung der Privatsphäre zustande kommen. Dabei können personenbezogene Informationen über Teilnehmende des Trainingsdatensatzes oder auch vertrauliche Merkmale der Daten exponiert werden.
\subsubsection{Angriffsvektoren}
Im Nachfolgenden werden potenzielle Angriffsvektoren beschrieben, die im Rahmen der Ausführung eines Angriffs genutzt werden, um optimale Ergebnisse zu erzielen.

Die einfachste Methode, die keine Informationen über verwendete Daten und interne Modellstrukturen erfordert, besteht in der Analyse der Modellausgabe im Hinblick auf bestimmte Eingabedaten. Dadurch können Angreifer Rückschlüsse auf innere Strukturen, Muster und Merkmale ziehen. Dies wird durch die Beobachtung von Wahrscheinlichkeiten oder Genauigkeiten der Modellausgabe auf der Grundlage bestimmter Eingaben $I$ erreicht.

Ein weiterer Angriffsvektor für die Durchführung wird durch interne Informationen des Modells dargestellt. Dies kann beispielsweiße durch die zugrunde liegenden Gradienten der Verlustfunktion geschehen. Dieser Gradienten-basierte Vektor ermöglicht dem Angreifer eine Approximation der Genauigkeit durch einen bestimmten Iterationsprozess basierend auf einen Input $I$. Dieses Verfahren erfordert oft Gradienteninformationen des Modells. 

In Black-Box Szenarien greift man auf den Vektor der Modellausgabe zurück. Hierbei versucht der Angreifer iterativ den Input $I$ zu verändern, sodass die Genauigkeit des Modells bezüglich der Zielklasse maximiert wird. 
\subsubsection{Verteidigungsstrategien}{\label{diff_privacy}}
Verteidigungsmaßnahmen gegenüber Modell-Inversionsangriffen sind von entscheidender Bedeutung, um die Privatsphäre und Sicherheit von neuronalen Netzwerken zu gewährleisten. Durch die Implementierung geeigneter Schutzmaßnahmen können potenzielle Angreifer daran gehindert werden, auf sensible Informationen der Trainingsdaten zuzugreifen und diese zu rekonstruieren.

Diese Verteidigungsmaßnahmen zielen darauf ab, die Effektivität von Modell-Inversionsangriffen bei nahezu gleichbleibender Modellleistung zu reduzieren und den Informationsverlust über Trainingsdaten zu minimieren. Dabei werden verschiedene Techniken eingesetzt, um den Rekonstruktionsprozess durch die Minimierung der Angriffsvektoren zu erschweren und den Angreifer von einer erfolgreichen Analyse der Modellausgaben abzuhalten.

Im Kapitel \ref{chpt:DefenseMI} dieser Arbeit werden konkrete Verteidigungsstrategien anderer Forschungsarbeiten gegenüber Modell-Inversionsangriffen eingehend beleuchtet, beschrieben und bewertet. Die strategischen Ansätze werden dabei darauf abzielen, sowohl die Ausgabewerte des Modells als auch interne Modellinformationen zu schützen, um die Extraktion sensibler Trainingsdaten zu vereiteln. Durch die Beschreibung detaillierter Untersuchungen dieser Verteidigungsstrategien wird eine umfassende Einsicht in bewährte Praktiken zur Abschwächung von Modell-Inversionsangriffen ermöglicht.

Die Ergebnisse der Implementierung von differentieller Privatsphäre, einer Verteidigungsmaßnahme, die durch Verhinderung von Overfitting und das Einfügen von Rauschen im Trainingsprozess auf eine geschütztere Modellentwicklung abzielt, sind in Kapitel \ref{chpt:dpnn_stats} beschrieben und dargestellt.
