% Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures (Frederickson)
%Algorithms that remember: model inversion attacks and data protection law
\section{Grundlegende Konzepte und Modelle}\label{chpt:Stand_der_Technik_MI}
%Vorstellung grundlegender Konzepte und Modelle im Zusammenhang mit Model Inversion Attacks.
Mit stetigem Wandel und der Weiterentwicklung digitaler Systeme gewinnt die Sicherheit und Robustheit zunehmend an Bedeutung. Insbesondere mit der vermehrten Integration von Künstlicher Intelligenz (KI) ergeben sich neue Herausforderungen im Kontext der Sicherheits- und Robustheitsaspekte. Dieses Unterkapitel widmet sich den grundlegenden Aspekten des Modell-Inversionsangriffs, einem Schlüsselelement in der Diskussion um die Robustheit neuronaler Netzwerke.

Die fundamentale Idee dieses Angriffsszenarios manifestiert sich durch die Rekonstruktion von im Trainingsprozess verwendeten Datenpunkten, wodurch die Privatsphäre einzelner Individuen des Trainingsdatensatzes kompromittiert wird. Dabei geht es um den gezielten Versuch, sensible Informationen durch eine geschickte Interaktion mit dem Modell zu extrahieren und somit die Integrität der Trainingsdaten zu durchbrechen (siehe \ref{subsec: MIAttack}). 

Die Durchführung einer Modell-Inversionsattacke kann basierend auf unterschiedlichen Modellarchitekturen stattfinden. Das Paper \glqq Privacy in Pharmacogenetics\grqq{} (\cite{fredrikson_privacy_2014}) befasst sich mit der Angriffsdurchführung auf Basis eines \textit{Regressionsmodells} unter anderem im WhiteBox-Setting. Hierbei sind dem Angreifer, der mit dem Ziel der Vorhersage des Genotyps eines bestimmten Individums auf Basis verschiedener Merkmale und Attribute der Person handelt, Modellstrukturen und -parameter bekannt (\cite[20]{fredrikson_privacy_2014}). Das Hauptziel des Angriffs beschreibt das Erlangen sensibler Informationen, was die Verletzung der Privatsphäre einzelner Personen mit sich bringt. Mit Hilfe des Algorithmus, der für die Durchführung der Modell-Inversionsattacke programmiert wurde und unabhängig von zugrundeliegenden Modellstrukturen ist, kann durch die Invertierung des Modells auf genetische Marker der Person Rückschluss gezogen werden. Dem Autor zur Folge sei der verallgemeinerte Modellinversionsalgorithmus optimal, da dieser die erwartete Fehlervorhersage des Angreifers minimiert. Zudem sei der Algorithmus nahezu so effektiv wie speziell trainierte Regressionsmodelle, die auf die Vorhersage dieser Marker abzielen. Neben den Modell-Inversionsangriffen werden auch Verteidigungsmechanismen beschrieben, die zur Sicherung der Privatsphäre beitragen kann. Einer der Algorithmen ist die Integration von differentieller Privatsphäre. Das genutzte Regressionsmodell ist auf Basis des IWPC Datensatzes trainiert und wird für die Durchführung und Analyse von Angriffsszenarios, sowie Auswertung der Verteidigungsstrategie verwendet.

Eine weitere Modellarchitektur, die eine Anfälligkeit gegenüber Modell-Inversionsangriffen bietet, sind Entscheidungsbäume (\cite[S. 4 ff.]{fredrikson_model_2015}), welche vom Autor als Modelle beschrieben werden, die den Merkmalsraum in disjunkte Regionen partitioniert und Vorhersagen für Instanzen basierend auf Merkmalen macht. Dabei beschreibt er, dass zwischen \textit{Klassifikationsbäumen}, deren Zielvariable diskret, und \textit{Regressionsbäumen}, deren Zielvariable kontinuierlich ist, unterschieden wird. In Anbetracht auf Modell-Inversionsangriffe nutzt der Autor Black- und Whitebox Settings für die Durchführung und Evaluierung der Angriffe. Im BlackBox Setting wird eine angepasste Form des Algoritmus für Regressionsmodelle (\cite[21]{fredrikson_privacy_2014}) verwendet, der auf die Grundarchitektur und der veränderten Ausgabe der Entscheidungsbäume zurückzuführen ist. Die WhiteBox-Methode hingegen appliziert Konfidenzwerte zur Inferenz sensibler Informationen, indem sie die Konfidenzwerte verschiedener Klassifikationen analysiert, um daraus Schlüsse auf potenzielle Werte der sensiblen Merkmale zu ziehen. Durch das bereitgestellte Wissen über interne Modellstrukturen besteht die Möglichkeit der Inferenz sensibler Informationen mit hoher Präzision. Besonders für Datenpunkte, die Teil des Trainingsdatensatzes sind. 

Eine wichtige Rolle im Bereich der Inversions-Angriffe spielt die Modellarchitektur der neuronalen Netzwerke zur Klassifikation von Bildern. Diese dienen als Klassifikator für ungesehene Eingaben und eröffnen dem Angreifer die Möglichkeit zur Rekonstruktion der im Training verwendeten Datenpunkte. Das Paper \glqq Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures\grqq{} (\cite[7]{fredrikson_model_2015}) legt das Augenmerk auf Modelle zur Gesichtsidentifikation und nutzt jeweils ein \textit{Softmax-Regressionsmodell}, \textit{MLP} und \textit{SDAE-Modell} zur Durchführung von Modelinversionsangriffen, wobei alle Architekturen einen vergleichsweise einfachen Aufbau vorweisen. Das Softmax-Regressionsmodell ist mit einer generalisierten Version eines logistischen Regressionsmodell zu beschreiben, wobei die Klassenvariable mehr als 2 Werte annehmen kann. Da der zugrundeliegende Datensatz aus 40 Klassen besteht, kann ein austrainiertes Regressionsmodell zwischen 40 verschiedenen Werten für die Klassenvariable unterscheiden. Der Hauptunterschied zwischen den genannten Modellen  besteht lediglich in der Anzahl verborgener Schichten. Das MLP weist genau eine verborgene Schicht auf, wohingegehen das Softmax-Regressionsmodell keine besitzt. Das SDAE-Modell weist sogar 2 verborgene, vollvermaschte Schichten, die als \glqq Denoising Autoencoder\grqq{} verwendet werden, auf und ist damit in der Lage besser Merkmale und Muster in den Eingabedaten zu erfassen. Das Paper befasst sich auf Basis der genannten Modelle mit der Ausführung einer WhiteBox-basierten Modell-Inversionsattacke, die ganze Bilder mit den jeweiligen Pixelwerten versucht zu rekonstruieren. Dies geschieht auf Basis des beschriebenen Algorithmus (\cite[8]{fredrikson_model_2015}), der den Gradientenabstieg für die Miminierung der Kostenfunktion verwendet. Ziel der Arbeit ist die Untermalung der Gefahr von Modell-Inversionsangriffen auf Basis verschiedener Modellarchitekturen.

Die jeweiligen Erkenntnisse und Ergebnisse der aufgezeigten Forschungsbeiträge werden im Folgekapitel bleuchtet.
\section{Forschungsergebnisse}\label{chpt:MIAttacksResearch}
%Zusammenfassung der wichtigsten Forschungsergebnisse und Erkenntnisse im Bereich Model Inversion Attacks.
Die Ergebnisse der Netzwerke zur Vorhersage von Genotypen (\cite[S. 7 ff.]{fredrikson_privacy_2014}) werden auf Basis des linearen Regressionsmodells durch die Ausführung des im Paper beschriebenen Algorithmus zur Durchführung der Modell-Inversionsattacke bereitgestellt und mit Vorhersagen eines optimalisierten logistischen Regressionsmodell verglichen. Die Qualitätsbewertung des Angriffs erfolgt hierbei zum einen mittels \glqq AUCROC\grqq{} Kurve, sowie auf Basis der Genauigkeit von verschiedenen Modellen und Idealen. Um die Effektivität und Qualität des Angriffs bewerten zu können, nutzt Frederikson eine Vergleichsmethode, welche Algorithmik-Vorhersagen, Ideal-Vorhersagen und BaseLine-Genauigkeit gegenüberstellt. Dabei beschreibt die BaseLine-Genauigkeit diese, welche durch Raten des wahrscheinlichsten Genotyps auf Basis der gegebenen marginalen Priorität, die für die jeweilige Klasse variiert, erzielt wird.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{Bilder/paper_fred_2014_graph.png}
	\caption{Ergebnisse: Modell-Inversionsangriff (\cite[S. 7, Figure 3]{fredrikson_privacy_2014})}
	\label{img:frederikson_evaluation}
\end{figure}
Gemäß Abbildung \ref{img:frederikson_evaluation}, entnommen aus dem veröffentlichten Paper von Frederikson, zeigt sich durch die Analyse der AUCROC-Kurve, dass der Algorithmus (Modell-Inversion) in der Lage ist, VKORC1-Genotypen präziser vorherzusagen als das Idealmodell. Dennoch weisen die durch den Algorithmus erzeugten Ergebnisse eine leicht geringere Genauigkeit im Vergleich zu vorab trainierten Idealmodellen auf, weshalb die Leistung des Angriffs geringfügig schlechter ist als die des ursprünglichen Modells.
Im Gegensatz zu VKORC1 kann für CYP2C9 aufgrund der Komplexität und der damit verbundenen Schwierigkeiten bei der Vorhersage weder mit Hilfe des Idealmodells noch des Algorithmus eine annähernd hohe Genauigkeit erreicht werden. Dies führt dazu, dass die Baseline-Genauigkeit deutlich höher ist als andere Genauigkeiten, und die Effizienz des Angriffs bei CYP2C9 im Vergleich zu VKORC1-Genotypen erheblich schlechter ist.
Nach der Durchführung wurde zudem die Genauigkeit der Modelle im Hinblick auf Trainings- und Validierungsgenauigkeit überprüft, wobei eine deutliche Verschlechterung im Bezug auf ungesehene Validierungsdaten festgestellt worden war.

Auf Basis der in Kapitel \ref{chpt:Stand_der_Technik_MI} dargestellten Entscheidungsbäumen wurden mit dem Ziel der Demonstration von Modell-Inversionsangriffen verschiedene Experimente durchgeführt. Der zugrunde liegende Datensatz ist dabei zum einen der \glqq Five-Thirty-Eight\grqq{}-Datensatz, welcher durch Umfragen erworben wurde und verschiedene Merkmale, wie demografische Informationen und Vorlieben, enthält. Zudem wurde der \glqq GSS marital happiness\grqq{}-Datensatz verwendet, der verschiedene Aspekte des Eheglücks beinhaltet. Zu den Mermalen gehören hierbei neben demografischen Informationen auch Interessen und Einstellungen von US-amerikanischen Einwohnern. Mit diesen Datensätzen als Grundlage wird versucht, sensible Informationen anhand Modell-Inversionsangriffen aus den Bäumen zu extrahieren. Die Angriffe, basierend auf spezifischen Algorithmen, wurde sowohl im WhiteBox- sowie im BlackBox-Setting für jede einzelne Zeile des Datensatzes durchgeführt. Die zugrundeliegenden Modelle wurden von \glqq Online-Anbietern\grqq{} bezogen, um das Risiko öffentlich verfügbarer Modelle zu bewerten. In der Auswertung (siehe Abbildung \ref{img:frederikson_2015_evaluation}) verschiedener Durchführungen (\cite[S. 5 ff.]{fredrikson_model_2015}) wird zum einen beschrieben, dass auf WhiteBox-basierte Inversionsangriffe eine hohe Präzision bezüglich positiver Instanzen der sensiblen Variablen zu erkennen ist. Somit kann ein Angreifer, der Zugriff auf das zugrundeliegende Modell besitzt, verhältnismäßig einfach sensible Informationen, wie \glqq Ja\grqq{}-Antworten auf bestimmte Fragen, extrahieren. Das Risiko der \glqq ungewollten Bereitstellung\grqq{} sensibler Informationen ist deutlich höher, wenn der Repräsentant Teil des Trainingsdatensatzes ist und dabei effektiv verwendet wurde. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{Bilder/frederikson_2015_1.png}
	\caption{MI-Ergebnisse für BigML-Modelle (\cite[S. 6, Figure 4]{fredrikson_model_2015})}
	\label{img:frederikson_2015_evaluation}
\end{figure}
Anhand der in Abbildung \ref{img:frederikson_2015_evaluation} dargestellten Angriffsergebnisse lässt sich erkennen, dass die Leistung des WhiteBox-Verfahrens, das keine falsch-positiven Ausgaben produziert, deutlich höher ist, als die des BlackBox-Verfahren. Verdeutlicht wird, dass beide Angriffe die Genauigkeit der Zufallsgenerierung übertreffen, kein Angriff aber einen höhere Recall aufgrund der verzerrten Verteilung des sensiblen Mermals erreichen konnte. In einer weiteren Abbildung (\cite[S. 7, Figure 5]{fredrikson_model_2015}) wird ein Vergleich zwischen Angriffsdurchführungen von Trainings- und Testdaten dargestellt, womit eine erhebliche Risikosteigerung für die Vertraulichkeit durch den Einbezug von Daten in den Trainingsprozess verdeutlicht wird. 

Die beiden Angriffe bezüglich der in Kapitel \ref{chpt:DefenseMI} dargestellten Modelle (\cite[S. 7 ff.]{fredrikson_model_2015}) zur Gesichtserkennung setzen unterschiedliches Wissen über Daten und Modelle voraus. Im ersten Angriff handelt es sich um ein Szenario in welchem der Angreifer das Label beziehungsweise einen eindeutigen Indikator für eine bestimmte Person kennt, um diese rekonstruieren zu können. Dabei kennt der Angreifer neben dem Ziel-Label keine weiteren Parameter und Strukturen. Mit Hilfe des im Paper definierten Angriffsalgorithmus (MI-Face) werden Bilder generiert, die durch eine Nachbearbeitungsfunktion (Process) qualitativ verbessert werden können. Bei der Verwendung des SDAE-Modells verwendet der Autor eine abgeänderte Process-Funktion (Process-DAE), um Kandidatenlösungen im latenten Raum der ersten Autoencoder-Schicht zu generieren. Diese Funktion dekodiert während jeder Iteration den Kandidaten in den ursprünglichen Pixelraum, wendet einen Entrauschungsfilter gefolgt von einem Schärfefilter an und kodiert die resultierenden Pixel in den latenten Raum zurück. Dadurch wird viel Rauschen aus der finalen Rekonstruktion entfernt. 
\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{Bilder/frederikson_2015_2.png}
	\caption{MI-Ergebnisse des \glqq Mechanical Turk Survey\grqq{} (\cite[S. 10, Figure 9]{fredrikson_model_2015})}
	\label{img:frederikson_2015_evaluation2}
\end{figure}
Die in Abbildung \ref{img:frederikson_2015_evaluation2} dargestellten Grafiken entstanden auf Basis eines bestimmten Experimentier- und Evaluierungskonzept, wobei Mechanical Turk-Arbeiter gebeten wurden, ein rekonstruiertes Bild einem von fünf Gesichtsbildern des AT\&T-Datensatzes zuzuordnen. Falls dieser der Meinung war, die Generierung würde keinem der gezeigten Bilder entsprechen, war dies auch eine mögliche Antwort. Dabei wurde jeder Satz der Experimente mit den selben Testbildern drei mal durchgeführt, um die Konsistenz der Arbeiterantworten zu untersuchen. Alle dem Turk-Arbeiter gezeigten Bilder entstammten aus dem Validierungsdatensatz, wodurch sie keinen Teil des Trainingsdatensatzes darstellten. Um die Grafik besser zu verdeutlichen folgen vorerst die Begrifferklärungen des Autors: \textit{overall} beschreibt das Ergebnis, wobei der Arbeiter alle Antworten korrekt gegeben hat. \textit{Identified} bezieht sich auf diejenigen Fälle, bei denen die Arbeiter die richtige Person, wenn diese Teil der Testbilder war, identifizieren konnte. \textit{Excluded} sind genau die Fälle, in denen Personenbilder der angegriffenen Label nicht angezeigt wurde und der Arbeiter korrekterweise kein Bild zugeordnet hat.
Der linke Plot aus Abbildung \ref{img:frederikson_2015_evaluation2} stellt die Ergebnisse über alle Antworten gemittelt da. Der mittlere Plot dahingegen visualisiert die Ergebnisse, bei denen mindestens zwei der drei Antworten korrekt waren. Der rechte Plot visualisiert die Ergebnisse geschulter Arbeiter, die sich durch mindestens fünf abgeschlossene Aufgaben mit einer Genauigkeit von 75\% oder mehr auszeichnen.
Mit Hilfe der Ergebnisdarstellungen lässt sich erkennen, dass Modell-Inversionsangriffe bezüglich des Softmax-Regressionsmodells am besten erraten wurden, wodurch man auf die genauste und qualitativ beste Repräsentation der Generierungen schließen kann. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{Bilder/frederikson_2015_3.png}
	\caption{MI-Generierungsqualität der verschiedenen Modelle (\cite[S. 10, Figure 10]{fredrikson_model_2015})}
	\label{img:frederikson_2015_evaluation3}
\end{figure}
Die Genauigkeitsunterschiede zwischen den verschiedenen Durchführungen lässt sich anhand der in Abbildung \ref{img:frederikson_2015_evaluation3} dargestellten Generierungsresultate eindeutig erklären. Bilder, die basierend auf dem Softmax-Regressionsmodell generiert wurden, sind schärfer und deutlicher zu erkennen, als die der anderen beiden Modelle.
\section{Gegenmaßnahmen und Abwehrstrategien}\label{chpt:DefenseMI}
%Vorstellung von Forschungsergebnissen zu Gegenmaßnahmen und Abwehrstrategien gegen Model Inversion Attacks.
Um gegen Angriffe vorzugehen und die Genauigkeit dieser signifikant verschlechtern zu können, nutzt man verschiedene Arten von Verteidigungsstrategien. Diese reichen von Sicherung des Datensatzes durch eine Störung bis hin zur gezielten Veränderung der Modellparameter im Training. 

Die erste Verteidigungsstrategie, die im Folgenden beschrieben wird, zeigt das Vorgehen zur Erstellung eines differentiell privaten Datensatzes anhand der im Paper beschriebenen \glqq Methode des privaten projizierten Histogramms\grqq{} (eng.: diffferentially-private histogram) (\cite[9]{fredrikson_privacy_2014}). Dem Autor zufolge sind Methoden zur Privatisierung von Datensätzen unter Entwicklern beliebter, da diese dadurch mehr Freiheit bei der Modellerstellung ohne die Integration interner Sicherheitsmechanismen haben. Für die Durchführung der Sicherheitsmaßnahme werden die numerischen Attribute des Datensatzes in gleichbreite Bins diskretisiert. Die Bin-Breite berechnet sich durch die im Paper beschriebene Heuristik ($(log(n)/n)^{1/(d+1)}$). Die Erstellung dieser privaten Datensätze basiert auf Parametern, die durch die Autoren innerhalb 100 vollständiger Generierungen bezüglich der Genauigkeit eines Regressionsmodells getestet wurden. Genaueres zu Herleitung genomischer Attribute aus einem privaten Datensatz ist der Publikation zu entnehmen (\cite[9]{fredrikson_privacy_2014}). Neben der Veränderung des zugrunde liegenden Datensatzes bringt Frederikson eine weitere Verteidigungsstrategie als Vergleichsbasis ein, in welcher die Privatisierung des linearen Regressionsmodells durchgeführt wird. Durch das Hinzufügen von Laplace-Rauschen zu den Koeffizienten der Ziel-Funktion wird die Privatisierung der lineare Regression gesteuert und die Durchführung von Modell-Inversionsangriffen erschwert (\cite[9]{fredrikson_privacy_2014}). Der Autor beschreibt die Erstellung dieser Modelle durch eine Projizierung der Spaltenwerte auf einen Wert zwischen -1 und 1. Daraufhin werden alle Werte, bis auf die der Zielkategorie, einheitlich skaliert. Durch einen Vergleich beider Verteidigungsstrategien haben die Autoren anhand der AUCROC-Metrik dargestellt, dass private Datensätze signifikant mehr Informationen über sensible Daten preisgeben, als privat trainierte Regressionsmodelle. Allerdings wurde durch die verschiedenen Testläufe auch ersichtlich, dass unterschiedliche Leistung anhand verschiedener Parameter eine geeignete Wahl des jeweiligen Mechanismus bei der Anwendung von differentieller Privatsphäre benötigt.

Neben den genannten Verteidigungsansätzen wird eine weitere Methode vorgestellt, die auf Basis von \glqq adversarialer Angriffe\grqq{} einen spezifischen Rauschvektor erstellt, um die Ausgabe des Zielmodells so zu verändern, dass der Rekonstruktions-Verlust von BlackBox-basierten Modell-Inversionsangriffen maximiert wird. Dieser Ansatz (\cite{wen_defending_2021}) zielt im Gegensatz zu den oben genannten Verteidigungsverfahren auf komplexere Modellarchitekturen, wie CNNs, ab. Insgesamt ist das Ziel dieser Verteidigungsmethode der Schutz des neuronalen Netzwerks vor Modell-Inversionsangriffen, die auf Basis adversarialer Prinzipen durchgeführt werden. Die Implementierung startet mit dem einfachen Training eines Modells zur Durchführung einer Modell-Inversionsattacke, dessen Gradient für die Berrechnung des Rauschvektors verwendet wird. Diese wird auf Basis des dargestellten Algorithmus (\cite[S. 4, Algorithm 1]{wen_defending_2021}) mit dem Ziel der Verlustmaximierung des Angriffsmodell durchgeführt. Nach mehrfacher Optimierung des Rauschvektors innerhalb des eigentlichen Verteidigungsalgorithmus (\cite[S. 4, Algorithm 2]{wen_defending_2021}) kann dieser durch Anwendung die Ausgabe des Modells so verändern, dass Modell-Inversionsangriffe mit deutlich schwächerem Erfolg ausgeführt werden können. Anhand der Genauigkeit des \glqq verteidigten\grqq{} Modells bezüglich verschiedener Angriffsszenarios und des Rekonstruktionsfehlers der Angriffsmodelle lässt sich die Effektivität der Verteidigungsstrategie bewerten. Durch die dargestellten Vergleiche zeigt der Autor die Genauigkeitsunterschiede der Modelle nach der Anwendung verschiedener Verteidigungsmechanismen. Dabei wird ersichtlich (\cite[S. 5, Table 1]{wen_defending_2021}), dass durch die Implementierung der hier dargestellten Verteidigungsstrategie die Genauigkeit des Modells der des Ausgangszustandes ohne jeglieche Verteidigungsmaßnahmen gleicht. Zudem erhöht sich der Rekonstruktionsfehler des Modell-Inversionsangriffs signifikant und deutet auf eine Abschwächung der Angriffsqualität hin. Die Annnahme, dass die Integration des Verteidigungsmechansimus die Effektivität und Qualität der Modell-Inversionsangriffe schwächt, belegt der Autor anhand einer Darstellung (\cite[S. 5, Fig. 1]{wen_defending_2021}), die einen Vergleich zwischen Rekonstruktionen, basierend unterschiedliche Modell-Setups, visualisiert. Erkenntlich wird damit die deutlich höhere Missklassifikationrate der Rekonstruktionen durch Anwendung der dargestellten Verteidigungsstrategie.