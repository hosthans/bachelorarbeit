\section{Aufgabenstellung} \label{chpt:Einleitung_Aufgabenstellung}

\textbf{Hintergrund}

Da KI-Systeme immer mehr Anwendung finden, werden diese häufiger Ziele von Cyber-Angriffen. Es gibt viele verschiedene Angriffsvektoren auf diese Systeme, zu denen auch das Erlangen von zugrundeliegenden Informationen über Daten des Trainingsprozesses von Neuronalen Netwerken gehört. Da viele Modelle mit sensiblen Daten trainiert werden, ist es wichtig, dass ein potentieller Angreifer durch das Modell keinen Zugruff auf Daten erlangen kann.
Der \glqq EU-AI Act\grqq{} enthält unter anderem Anforderungen an KI-Systeme in Bezug auf Robustheit und Cybersicherheit. Daher bringt dieser nicht nur die Verantwortung sichere Modelle zu trainieren mit sich, sondern auch die Sicherstellung, dass genutzte Trainingsdaten privat gehalten werden. Um Modelle während der Bereitstellung abzusichern, müssen die verschiedenen Angriffsvektoren und die entsprechenden Abwehrmaßnahmen bekannt sein.
\newline

\textbf{Ziel der Arbeit} 

Während der Thesis sollen folgende Fragen beantwortet werden:
\begin{itemize}
	\item Welche Angriffsmöglichkeiten gibt es, um Daten von deployten Modellen zu extrahieren?
	\item Wie kann man sich gegen diese Attacken schützen?
	\item Showcase zu Verteidigungstechniken und deren Effektivität gegenüber Inversions-Angriffen. \newline
\end{itemize}

\textbf{Methodischer Ansatz}

\begin{itemize}
	\item Onboarding
	\item Recherche über verschiedene Attacken und Verteidigungsmöglichkeiten
	\item Vergleich von verschiedenen Angriffen auf unterschiedliche Modell-Architekturen
	\item Implementierung eines Showcases für mindestens einen Angriff auf mindestens eine Modell-Architektur:
		\begin{itemize}
			\item Zeigen, wie ein solcher Angriff funktioniert.
			\item Kann man sich gegen einen solchen Angriff verteidigen?
			\item Wie effektiv sind die Verteidigungsstrategien? \newline
		\end{itemize}
\end{itemize}


