
@book{lorenz_reinforcement_2020,
	location = {Berlin, Heidelberg},
	title = {Reinforcement Learning: Aktuelle Ansätze verstehen - mit Beispielen in Java und Greenfoot},
	isbn = {978-3-662-61650-5 978-3-662-61651-2},
	url = {http://link.springer.com/10.1007/978-3-662-61651-2},
	shorttitle = {Reinforcement Learning},
	publisher = {Springer Berlin Heidelberg},
	author = {Lorenz, Uwe},
	urldate = {2023-10-04},
	date = {2020},
	langid = {german},
	doi = {10.1007/978-3-662-61651-2},
}

@book{joshi_machine_2020,
	location = {Cham},
	title = {Machine Learning and Artificial Intelligence},
	isbn = {978-3-030-26621-9 978-3-030-26622-6},
	url = {http://link.springer.com/10.1007/978-3-030-26622-6},
	publisher = {Springer International Publishing},
	author = {Joshi, Ameet V},
	urldate = {2023-11-24},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-26622-6},
}

@inproceedings{shahapure_cluster_2020,
	location = {sydney, Australia},
	title = {Cluster Quality Analysis Using Silhouette Score},
	isbn = {978-1-72818-206-3},
	url = {https://ieeexplore.ieee.org/document/9260048/},
	doi = {10.1109/DSAA49011.2020.00096},
	abstract = {Clustering is an important phase in data mining. Selecting the number of clusters in a clustering algorithm, e.g. choosing the best value of k in the various k-means algorithms [1], can be difﬁcult. We studied the use of silhouette scores and scatter plots to suggest, and then validate, the number of clusters we speciﬁed in running the k-means clustering algorithm on two publicly available data sets. Scikit-learn’s [4] silhouette score method, which is a measure of the quality of a cluster, was used to ﬁnd the mean silhouette co-efﬁcient of all the samples for different number of clusters. The highest silhouette score indicates the optimal number of clusters. We present several instances of utilizing the silhouette score to determine the best value of k for those data sets.},
	eventtitle = {2020 {IEEE} 7th International Conference on Data Science and Advanced Analytics ({DSAA})},
	pages = {747--748},
	booktitle = {2020 {IEEE} 7th International Conference on Data Science and Advanced Analytics ({DSAA})},
	publisher = {{IEEE}},
	author = {Shahapure, Ketan Rajshekhar and Nicholas, Charles},
	urldate = {2023-11-28},
	date = {2020-10},
	langid = {english},
	file = {Shahapure und Nicholas - 2020 - Cluster Quality Analysis Using Silhouette Score.pdf:/Users/hannes/Zotero/storage/LCZ2BTG6/Shahapure und Nicholas - 2020 - Cluster Quality Analysis Using Silhouette Score.pdf:application/pdf},
}

@collection{balas_recent_2020,
	location = {Cham},
	title = {Recent Trends and Advances in Artificial Intelligence and Internet of Things},
	volume = {172},
	isbn = {978-3-030-32643-2 978-3-030-32644-9},
	url = {http://link.springer.com/10.1007/978-3-030-32644-9},
	series = {Intelligent Systems Reference Library},
	publisher = {Springer International Publishing},
	editor = {Balas, Valentina E. and Kumar, Raghvendra and Srivastava, Rajshree},
	urldate = {2023-11-29},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-32644-9},
}

@online{noauthor_nvlabsffhq-dataset_2023,
	title = {{NVlabs}/ffhq-dataset},
	url = {https://github.com/NVlabs/ffhq-dataset},
	abstract = {Flickr-Faces-{HQ} Dataset ({FFHQ})},
	urldate = {2023-12-12},
	date = {2023-12-11},
	note = {original-date: 2019-02-04T15:35:08Z},
}

@online{noauthor_celeba_nodate,
	title = {{CelebA} Dataset},
	url = {https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html},
	urldate = {2023-12-18},
	file = {CelebA Dataset:/Users/hannes/Zotero/storage/MVN4M7TR/CelebA.html:text/html},
}

@online{noauthor_mnist_nodate,
	title = {{MNIST} — Torchvision main documentation},
	url = {https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html},
	urldate = {2023-12-18},
	file = {MNIST — Torchvision main documentation:/Users/hannes/Zotero/storage/EW3CS42P/torchvision.datasets.MNIST.html:text/html},
}

@misc{simonyan_very_2015,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our {ImageNet} Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisation and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing {ConvNet} models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	number = {{arXiv}:1409.1556},
	publisher = {{arXiv}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	urldate = {2023-12-19},
	date = {2015-04-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Simonyan und Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:/Users/hannes/Zotero/storage/L56MLLRV/Simonyan und Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf},
}

@misc{radford_unsupervised_2016,
	title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks ({CNNs}) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with {CNNs} has received less attention. In this work we hope to help bridge the gap between the success of {CNNs} for supervised learning and unsupervised learning. We introduce a class of {CNNs} called deep convolutional generative adversarial networks ({DCGANs}), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	number = {{arXiv}:1511.06434},
	publisher = {{arXiv}},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	urldate = {2023-12-19},
	date = {2016-01-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1511.06434 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:/Users/hannes/Zotero/storage/IA8GEYRF/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf},
}

@misc{karras_style-based_2019,
	title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1812.04948},
	abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-speciﬁc control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
	number = {{arXiv}:1812.04948},
	publisher = {{arXiv}},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	urldate = {2023-12-20},
	date = {2019-03-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1812.04948 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Karras et al. - 2019 - A Style-Based Generator Architecture for Generativ.pdf:/Users/hannes/Zotero/storage/N63BV4RT/Karras et al. - 2019 - A Style-Based Generator Architecture for Generativ.pdf:application/pdf},
}

@misc{chen_knowledge-enriched_2021,
	title = {Knowledge-Enriched Distributional Model Inversion Attacks},
	url = {http://arxiv.org/abs/2010.04092},
	abstract = {Model inversion ({MI}) attacks are aimed at reconstructing training data from model parameters. Such attacks have triggered increasing concerns about privacy, especially given a growing number of online model repositories. However, existing {MI} attacks against deep neural networks ({DNNs}) have large room for performance improvement. We present a novel inversion-speciﬁc {GAN} that can better distill knowledge useful for performing attacks on private models from public data. In particular, we train the discriminator to differentiate not only the real and fake samples but the soft-labels provided by the target model. Moreover, unlike previous work that directly searches for a single data point to represent a target class, we propose to model a private data distribution for each target class. Our experiments show that the combination of these techniques can signiﬁcantly boost the success rate of the state-of-the-art {MI} attacks by 150\%, and generalize better to a variety of datasets and models. Our code is available at https://github.com/{SCccc}21/Knowledge-Enriched-{DMI}.},
	number = {{arXiv}:2010.04092},
	publisher = {{arXiv}},
	author = {Chen, Si and Kahla, Mostafa and Jia, Ruoxi and Qi, Guo-Jun},
	urldate = {2023-12-21},
	date = {2021-08-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2010.04092 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Chen et al. - 2021 - Knowledge-Enriched Distributional Model Inversion .pdf:/Users/hannes/Zotero/storage/7DRMLYAI/Chen et al. - 2021 - Knowledge-Enriched Distributional Model Inversion .pdf:application/pdf},
}

@misc{nguyen_re-thinking_2023,
	title = {Re-thinking Model Inversion Attacks Against Deep Neural Networks},
	url = {http://arxiv.org/abs/2304.01669},
	abstract = {Model inversion ({MI}) attacks aim to infer and reconstruct private training data by abusing access to a model. {MI} attacks have raised concerns about the leaking of sensitive information (e.g. private face images used in training a face recognition system). Recently, several algorithms for {MI} have been proposed to improve the attack performance. In this work, we revisit {MI}, study two fundamental issues pertaining to all state-of-the-art ({SOTA}) {MI} algorithms, and propose solutions to these issues which lead to a significant boost in attack performance for all {SOTA} {MI}. In particular, our contributions are two-fold: 1) We analyze the optimization objective of {SOTA} {MI} algorithms, argue that the objective is sub-optimal for achieving {MI}, and propose an improved optimization objective that boosts attack performance significantly. 2) We analyze “{MI} overfitting”, show that it would prevent reconstructed images from learning semantics of training data, and propose a novel “model augmentation” idea to overcome this issue. Our proposed solutions are simple and improve all {SOTA} {MI} attack accuracy significantly. E.g., in the standard {CelebA} benchmark, our solutions improve accuracy by 11.8\% and achieve for the first time over 90\% attack accuracy. Our findings demonstrate that there is a clear risk of leaking sensitive information from deep learning models. We urge serious consideration to be given to the privacy implications. Our code, demo, and models are available at https://ngoc- nguyen- 0.github.io/rethinking\_model\_inversion\_attacks/.},
	number = {{arXiv}:2304.01669},
	publisher = {{arXiv}},
	author = {Nguyen, Ngoc-Bao and Chandrasegaran, Keshigeyan and Abdollahzadeh, Milad and Cheung, Ngai-Man},
	urldate = {2023-12-21},
	date = {2023-06-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2304.01669 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Nguyen et al. - 2023 - Re-thinking Model Inversion Attacks Against Deep N.pdf:/Users/hannes/Zotero/storage/6JTE9QQN/Nguyen et al. - 2023 - Re-thinking Model Inversion Attacks Against Deep N.pdf:application/pdf},
}

@misc{ye_one_2022,
	title = {One Parameter Defense -- Defending against Data Inference Attacks via Differential Privacy},
	url = {http://arxiv.org/abs/2203.06580},
	abstract = {Machine learning models are vulnerable to data inference attacks, such as membership inference and model inversion attacks. In these types of breaches, an adversary attempts to infer a data record’s membership in a dataset or even reconstruct this data record using a conﬁdence score vector predicted by the target model. However, most existing defense methods only protect against membership inference attacks. Methods that can combat both types of attacks require a new model to be trained, which may not be time-efﬁcient. In this paper, we propose a differentially private defense method that handles both types of attacks in a time-efﬁcient manner by tuning only one parameter, the privacy budget. The central idea is to modify and normalize the conﬁdence score vectors with a differential privacy mechanism which preserves privacy and obscures membership and reconstructed data. Moreover, this method can guarantee the order of scores in the vector to avoid any loss in classiﬁcation accuracy. The experimental results show the method to be an effective and timely defense against both membership inference and model inversion attacks with no reduction in accuracy.},
	number = {{arXiv}:2203.06580},
	publisher = {{arXiv}},
	author = {Ye, Dayong and Shen, Sheng and Zhu, Tianqing and Liu, Bo and Zhou, Wanlei},
	urldate = {2023-12-22},
	date = {2022-03-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2203.06580 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Ye et al. - 2022 - One Parameter Defense -- Defending against Data In.pdf:/Users/hannes/Zotero/storage/5HMMZJCF/Ye et al. - 2022 - One Parameter Defense -- Defending against Data In.pdf:application/pdf},
}

@online{noauthor_opacus_nodate,
	title = {Opacus · Train {PyTorch} models with Differential Privacy},
	url = {https://opacus.ai/},
	abstract = {Train {PyTorch} models with Differential Privacy},
	urldate = {2023-12-23},
	file = {Snapshot:/Users/hannes/Zotero/storage/IFXYWE3L/opacus.ai.html:text/html},
}

@inproceedings{abadi_deep_2016,
	title = {Deep Learning with Differential Privacy},
	url = {http://arxiv.org/abs/1607.00133},
	doi = {10.1145/2976749.2978318},
	abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a reﬁned analysis of privacy costs within the framework of diﬀerential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training eﬃciency, and model quality.},
	pages = {308--318},
	booktitle = {Proceedings of the 2016 {ACM} {SIGSAC} Conference on Computer and Communications Security},
	author = {Abadi, Martín and Chu, Andy and Goodfellow, Ian and {McMahan}, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
	urldate = {2023-12-27},
	date = {2016-10-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1607.00133 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Abadi et al. - 2016 - Deep Learning with Differential Privacy.pdf:/Users/hannes/Zotero/storage/7Q4ZZB7Q/Abadi et al. - 2016 - Deep Learning with Differential Privacy.pdf:application/pdf},
}

@misc{fredrikson_privacy_2014,
	title = {Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing},
	abstract = {We initiate the study of privacy in pharmacogenetics, wherein machine learning models are used to guide medical treatments based on a patient’s genotype and background. Performing an in-depth case study on privacy in personalized warfarin dosing, we show that suggested models carry privacy risks, in particular because attackers can perform what we call model inversion: an attacker, given the model and some demographic information about a patient, can predict the patient’s genetic markers.},
	author = {Fredrikson, Matthew and Lantz, Eric and Jha, Somesh and Lin, Simon and Page, David and Ristenpart, Thomas},
	date = {2014-08-20},
	langid = {english},
	file = {Fredrikson et al. - Privacy in Pharmacogenetics An End-to-End Case St.pdf:/Users/hannes/Zotero/storage/VCQU8NNF/Fredrikson et al. - Privacy in Pharmacogenetics An End-to-End Case St.pdf:application/pdf},
}

@inproceedings{fredrikson_model_2015,
	location = {Denver Colorado {USA}},
	title = {Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures},
	isbn = {978-1-4503-3832-5},
	url = {https://dl.acm.org/doi/10.1145/2810103.2813677},
	doi = {10.1145/2810103.2813677},
	abstract = {Machine-learning ({ML}) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classiﬁers in personalized medicine by Fredrikson et al. [13], adversarial access to an {ML} model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown.},
	eventtitle = {{CCS}'15: The 22nd {ACM} Conference on Computer and Communications Security},
	pages = {1322--1333},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSAC} Conference on Computer and Communications Security},
	publisher = {{ACM}},
	author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
	urldate = {2024-01-02},
	date = {2015-10-12},
	langid = {english},
	file = {Fredrikson et al. - 2015 - Model Inversion Attacks that Exploit Confidence In.pdf:/Users/hannes/Zotero/storage/R7GAQI8Y/Fredrikson et al. - 2015 - Model Inversion Attacks that Exploit Confidence In.pdf:application/pdf},
}

@misc{han_reinforcement_2023,
	title = {Reinforcement Learning-Based Black-Box Model Inversion Attacks},
	url = {http://arxiv.org/abs/2304.04625},
	abstract = {Model inversion attacks are a type of privacy attack that reconstructs private data used to train a machine learning model, solely by accessing the model. Recently, white-box model inversion attacks leveraging Generative Adversarial Networks ({GANs}) to distill knowledge from public datasets have been receiving great attention because of their excellent attack performance. On the other hand, current blackbox model inversion attacks that utilize {GANs} suffer from issues such as being unable to guarantee the completion of the attack process within a predetermined number of query accesses or achieve the same level of performance as whitebox attacks. To overcome these limitations, we propose a reinforcement learning-based black-box model inversion attack. We formulate the latent space search as a Markov Decision Process ({MDP}) problem and solve it with reinforcement learning. Our method utilizes the confidence scores of the generated images to provide rewards to an agent. Finally, the private data can be reconstructed using the latent vectors found by the agent trained in the {MDP}. The experiment results on various datasets and models demonstrate that our attack successfully recovers the private information of the target model by achieving state-of-the-art attack performance. We emphasize the importance of studies on privacy-preserving machine learning by proposing a more advanced black-box model inversion attack.},
	number = {{arXiv}:2304.04625},
	publisher = {{arXiv}},
	author = {Han, Gyojin and Choi, Jaehyun and Lee, Haeil and Kim, Junmo},
	urldate = {2024-01-05},
	date = {2023-04-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2304.04625 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Han et al. - 2023 - Reinforcement Learning-Based Black-Box Model Inver.pdf:/Users/hannes/Zotero/storage/47VD5AQX/Han et al. - 2023 - Reinforcement Learning-Based Black-Box Model Inver.pdf:application/pdf},
}

@online{weber_hosthansba_code_2024,
	title = {hosthans/ba\_code},
	url = {https://github.com/hosthans/ba_code},
	abstract = {Code-Implementation for bachelor thesis.},
	titleaddon = {{GitHub}},
	author = {Weber, Hannes},
	urldate = {2024-01-05},
	date = {2024-01-06},
	langid = {english},
}
