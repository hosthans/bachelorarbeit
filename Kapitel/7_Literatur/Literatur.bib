
@book{lorenz_reinforcement_2020,
	location = {Berlin, Heidelberg},
	title = {Reinforcement Learning: Aktuelle Ansätze verstehen - mit Beispielen in Java und Greenfoot},
	isbn = {978-3-662-61650-5 978-3-662-61651-2},
	url = {http://link.springer.com/10.1007/978-3-662-61651-2},
	shorttitle = {Reinforcement Learning},
	publisher = {Springer Berlin Heidelberg},
	author = {Lorenz, Uwe},
	urldate = {2023-10-04},
	date = {2020},
	langid = {german},
	doi = {10.1007/978-3-662-61651-2},
}

@book{joshi_machine_2020,
	location = {Cham},
	title = {Machine Learning and Artificial Intelligence},
	isbn = {978-3-030-26621-9 978-3-030-26622-6},
	url = {http://link.springer.com/10.1007/978-3-030-26622-6},
	publisher = {Springer International Publishing},
	author = {Joshi, Ameet V},
	urldate = {2023-11-24},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-26622-6},
}

@inproceedings{shahapure_cluster_2020,
	location = {sydney, Australia},
	title = {Cluster Quality Analysis Using Silhouette Score},
	isbn = {978-1-72818-206-3},
	url = {https://ieeexplore.ieee.org/document/9260048/},
	doi = {10.1109/DSAA49011.2020.00096},
	abstract = {Clustering is an important phase in data mining. Selecting the number of clusters in a clustering algorithm, e.g. choosing the best value of k in the various k-means algorithms [1], can be difﬁcult. We studied the use of silhouette scores and scatter plots to suggest, and then validate, the number of clusters we speciﬁed in running the k-means clustering algorithm on two publicly available data sets. Scikit-learn’s [4] silhouette score method, which is a measure of the quality of a cluster, was used to ﬁnd the mean silhouette co-efﬁcient of all the samples for different number of clusters. The highest silhouette score indicates the optimal number of clusters. We present several instances of utilizing the silhouette score to determine the best value of k for those data sets.},
	eventtitle = {2020 {IEEE} 7th International Conference on Data Science and Advanced Analytics ({DSAA})},
	pages = {747--748},
	booktitle = {2020 {IEEE} 7th International Conference on Data Science and Advanced Analytics ({DSAA})},
	publisher = {{IEEE}},
	author = {Shahapure, Ketan Rajshekhar and Nicholas, Charles},
	urldate = {2023-11-28},
	date = {2020-10},
	langid = {english},
	file = {Shahapure und Nicholas - 2020 - Cluster Quality Analysis Using Silhouette Score.pdf:/Users/hannes/Zotero/storage/LCZ2BTG6/Shahapure und Nicholas - 2020 - Cluster Quality Analysis Using Silhouette Score.pdf:application/pdf},
}

@collection{balas_recent_2020,
	location = {Cham},
	title = {Recent Trends and Advances in Artificial Intelligence and Internet of Things},
	volume = {172},
	isbn = {978-3-030-32643-2 978-3-030-32644-9},
	url = {http://link.springer.com/10.1007/978-3-030-32644-9},
	series = {Intelligent Systems Reference Library},
	publisher = {Springer International Publishing},
	editor = {Balas, Valentina E. and Kumar, Raghvendra and Srivastava, Rajshree},
	urldate = {2023-11-29},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-32644-9},
}

@online{noauthor_nvlabsffhq-dataset_2023,
	title = {{NVlabs}/ffhq-dataset},
	url = {https://github.com/NVlabs/ffhq-dataset},
	abstract = {Flickr-Faces-{HQ} Dataset ({FFHQ})},
	urldate = {2023-12-12},
	date = {2023-12-11},
	note = {original-date: 2019-02-04T15:35:08Z},
}

@online{noauthor_celeba_nodate,
	title = {{CelebA} Dataset},
	url = {https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html},
	urldate = {2023-12-18},
	file = {CelebA Dataset:/Users/hannes/Zotero/storage/MVN4M7TR/CelebA.html:text/html},
}

@online{noauthor_mnist_nodate,
	title = {{MNIST} — Torchvision main documentation},
	url = {https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html},
	urldate = {2023-12-18},
	file = {MNIST — Torchvision main documentation:/Users/hannes/Zotero/storage/EW3CS42P/torchvision.datasets.MNIST.html:text/html},
}

@misc{simonyan_very_2015,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our {ImageNet} Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisation and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing {ConvNet} models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	number = {{arXiv}:1409.1556},
	publisher = {{arXiv}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	urldate = {2023-12-19},
	date = {2015-04-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Simonyan und Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:/Users/hannes/Zotero/storage/L56MLLRV/Simonyan und Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf},
}

@misc{radford_unsupervised_2016,
	title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks ({CNNs}) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with {CNNs} has received less attention. In this work we hope to help bridge the gap between the success of {CNNs} for supervised learning and unsupervised learning. We introduce a class of {CNNs} called deep convolutional generative adversarial networks ({DCGANs}), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	number = {{arXiv}:1511.06434},
	publisher = {{arXiv}},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	urldate = {2023-12-19},
	date = {2016-01-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1511.06434 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:/Users/hannes/Zotero/storage/IA8GEYRF/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf},
}

@misc{karras_style-based_2019,
	title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1812.04948},
	abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-speciﬁc control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
	number = {{arXiv}:1812.04948},
	publisher = {{arXiv}},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	urldate = {2023-12-20},
	date = {2019-03-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1812.04948 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Karras et al. - 2019 - A Style-Based Generator Architecture for Generativ.pdf:/Users/hannes/Zotero/storage/N63BV4RT/Karras et al. - 2019 - A Style-Based Generator Architecture for Generativ.pdf:application/pdf},
}

@misc{chen_knowledge-enriched_2021,
	title = {Knowledge-Enriched Distributional Model Inversion Attacks},
	url = {http://arxiv.org/abs/2010.04092},
	abstract = {Model inversion ({MI}) attacks are aimed at reconstructing training data from model parameters. Such attacks have triggered increasing concerns about privacy, especially given a growing number of online model repositories. However, existing {MI} attacks against deep neural networks ({DNNs}) have large room for performance improvement. We present a novel inversion-speciﬁc {GAN} that can better distill knowledge useful for performing attacks on private models from public data. In particular, we train the discriminator to differentiate not only the real and fake samples but the soft-labels provided by the target model. Moreover, unlike previous work that directly searches for a single data point to represent a target class, we propose to model a private data distribution for each target class. Our experiments show that the combination of these techniques can signiﬁcantly boost the success rate of the state-of-the-art {MI} attacks by 150\%, and generalize better to a variety of datasets and models. Our code is available at https://github.com/{SCccc}21/Knowledge-Enriched-{DMI}.},
	number = {{arXiv}:2010.04092},
	publisher = {{arXiv}},
	author = {Chen, Si and Kahla, Mostafa and Jia, Ruoxi and Qi, Guo-Jun},
	urldate = {2023-12-21},
	date = {2021-08-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2010.04092 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Chen et al. - 2021 - Knowledge-Enriched Distributional Model Inversion .pdf:/Users/hannes/Zotero/storage/7DRMLYAI/Chen et al. - 2021 - Knowledge-Enriched Distributional Model Inversion .pdf:application/pdf},
}

@misc{nguyen_re-thinking_2023,
	title = {Re-thinking Model Inversion Attacks Against Deep Neural Networks},
	url = {http://arxiv.org/abs/2304.01669},
	abstract = {Model inversion ({MI}) attacks aim to infer and reconstruct private training data by abusing access to a model. {MI} attacks have raised concerns about the leaking of sensitive information (e.g. private face images used in training a face recognition system). Recently, several algorithms for {MI} have been proposed to improve the attack performance. In this work, we revisit {MI}, study two fundamental issues pertaining to all state-of-the-art ({SOTA}) {MI} algorithms, and propose solutions to these issues which lead to a significant boost in attack performance for all {SOTA} {MI}. In particular, our contributions are two-fold: 1) We analyze the optimization objective of {SOTA} {MI} algorithms, argue that the objective is sub-optimal for achieving {MI}, and propose an improved optimization objective that boosts attack performance significantly. 2) We analyze “{MI} overfitting”, show that it would prevent reconstructed images from learning semantics of training data, and propose a novel “model augmentation” idea to overcome this issue. Our proposed solutions are simple and improve all {SOTA} {MI} attack accuracy significantly. E.g., in the standard {CelebA} benchmark, our solutions improve accuracy by 11.8\% and achieve for the first time over 90\% attack accuracy. Our findings demonstrate that there is a clear risk of leaking sensitive information from deep learning models. We urge serious consideration to be given to the privacy implications. Our code, demo, and models are available at https://ngoc- nguyen- 0.github.io/rethinking\_model\_inversion\_attacks/.},
	number = {{arXiv}:2304.01669},
	publisher = {{arXiv}},
	author = {Nguyen, Ngoc-Bao and Chandrasegaran, Keshigeyan and Abdollahzadeh, Milad and Cheung, Ngai-Man},
	urldate = {2023-12-21},
	date = {2023-06-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2304.01669 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Nguyen et al. - 2023 - Re-thinking Model Inversion Attacks Against Deep N.pdf:/Users/hannes/Zotero/storage/6JTE9QQN/Nguyen et al. - 2023 - Re-thinking Model Inversion Attacks Against Deep N.pdf:application/pdf},
}

@misc{ye_one_2022,
	title = {One Parameter Defense -- Defending against Data Inference Attacks via Differential Privacy},
	url = {http://arxiv.org/abs/2203.06580},
	abstract = {Machine learning models are vulnerable to data inference attacks, such as membership inference and model inversion attacks. In these types of breaches, an adversary attempts to infer a data record’s membership in a dataset or even reconstruct this data record using a conﬁdence score vector predicted by the target model. However, most existing defense methods only protect against membership inference attacks. Methods that can combat both types of attacks require a new model to be trained, which may not be time-efﬁcient. In this paper, we propose a differentially private defense method that handles both types of attacks in a time-efﬁcient manner by tuning only one parameter, the privacy budget. The central idea is to modify and normalize the conﬁdence score vectors with a differential privacy mechanism which preserves privacy and obscures membership and reconstructed data. Moreover, this method can guarantee the order of scores in the vector to avoid any loss in classiﬁcation accuracy. The experimental results show the method to be an effective and timely defense against both membership inference and model inversion attacks with no reduction in accuracy.},
	number = {{arXiv}:2203.06580},
	publisher = {{arXiv}},
	author = {Ye, Dayong and Shen, Sheng and Zhu, Tianqing and Liu, Bo and Zhou, Wanlei},
	urldate = {2023-12-22},
	date = {2022-03-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2203.06580 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Ye et al. - 2022 - One Parameter Defense -- Defending against Data In.pdf:/Users/hannes/Zotero/storage/5HMMZJCF/Ye et al. - 2022 - One Parameter Defense -- Defending against Data In.pdf:application/pdf},
}

@online{noauthor_opacus_nodate,
	title = {Opacus · Train {PyTorch} models with Differential Privacy},
	url = {https://opacus.ai/},
	abstract = {Train {PyTorch} models with Differential Privacy},
	urldate = {2023-12-23},
	file = {Snapshot:/Users/hannes/Zotero/storage/IFXYWE3L/opacus.ai.html:text/html},
}

@inproceedings{abadi_deep_2016,
	title = {Deep Learning with Differential Privacy},
	url = {http://arxiv.org/abs/1607.00133},
	doi = {10.1145/2976749.2978318},
	abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a reﬁned analysis of privacy costs within the framework of diﬀerential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training eﬃciency, and model quality.},
	pages = {308--318},
	booktitle = {Proceedings of the 2016 {ACM} {SIGSAC} Conference on Computer and Communications Security},
	author = {Abadi, Martín and Chu, Andy and Goodfellow, Ian and {McMahan}, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
	urldate = {2023-12-27},
	date = {2016-10-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1607.00133 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Abadi et al. - 2016 - Deep Learning with Differential Privacy.pdf:/Users/hannes/Zotero/storage/7Q4ZZB7Q/Abadi et al. - 2016 - Deep Learning with Differential Privacy.pdf:application/pdf},
}

@misc{fredrikson_privacy_2014,
	title = {Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing},
	abstract = {We initiate the study of privacy in pharmacogenetics, wherein machine learning models are used to guide medical treatments based on a patient’s genotype and background. Performing an in-depth case study on privacy in personalized warfarin dosing, we show that suggested models carry privacy risks, in particular because attackers can perform what we call model inversion: an attacker, given the model and some demographic information about a patient, can predict the patient’s genetic markers.},
	author = {Fredrikson, Matthew and Lantz, Eric and Jha, Somesh and Lin, Simon and Page, David and Ristenpart, Thomas},
	date = {2014-08-20},
	langid = {english},
	file = {Fredrikson et al. - Privacy in Pharmacogenetics An End-to-End Case St.pdf:/Users/hannes/Zotero/storage/VCQU8NNF/Fredrikson et al. - Privacy in Pharmacogenetics An End-to-End Case St.pdf:application/pdf},
}

@inproceedings{fredrikson_model_2015,
	location = {Denver Colorado {USA}},
	title = {Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures},
	isbn = {978-1-4503-3832-5},
	url = {https://dl.acm.org/doi/10.1145/2810103.2813677},
	doi = {10.1145/2810103.2813677},
	abstract = {Machine-learning ({ML}) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classiﬁers in personalized medicine by Fredrikson et al. [13], adversarial access to an {ML} model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown.},
	eventtitle = {{CCS}'15: The 22nd {ACM} Conference on Computer and Communications Security},
	pages = {1322--1333},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSAC} Conference on Computer and Communications Security},
	publisher = {{ACM}},
	author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
	urldate = {2024-01-02},
	date = {2015-10-12},
	langid = {english},
	file = {Fredrikson et al. - 2015 - Model Inversion Attacks that Exploit Confidence In.pdf:/Users/hannes/Zotero/storage/R7GAQI8Y/Fredrikson et al. - 2015 - Model Inversion Attacks that Exploit Confidence In.pdf:application/pdf},
}

@misc{han_reinforcement_2023,
	title = {Reinforcement Learning-Based Black-Box Model Inversion Attacks},
	url = {http://arxiv.org/abs/2304.04625},
	abstract = {Model inversion attacks are a type of privacy attack that reconstructs private data used to train a machine learning model, solely by accessing the model. Recently, white-box model inversion attacks leveraging Generative Adversarial Networks ({GANs}) to distill knowledge from public datasets have been receiving great attention because of their excellent attack performance. On the other hand, current blackbox model inversion attacks that utilize {GANs} suffer from issues such as being unable to guarantee the completion of the attack process within a predetermined number of query accesses or achieve the same level of performance as whitebox attacks. To overcome these limitations, we propose a reinforcement learning-based black-box model inversion attack. We formulate the latent space search as a Markov Decision Process ({MDP}) problem and solve it with reinforcement learning. Our method utilizes the confidence scores of the generated images to provide rewards to an agent. Finally, the private data can be reconstructed using the latent vectors found by the agent trained in the {MDP}. The experiment results on various datasets and models demonstrate that our attack successfully recovers the private information of the target model by achieving state-of-the-art attack performance. We emphasize the importance of studies on privacy-preserving machine learning by proposing a more advanced black-box model inversion attack.},
	number = {{arXiv}:2304.04625},
	publisher = {{arXiv}},
	author = {Han, Gyojin and Choi, Jaehyun and Lee, Haeil and Kim, Junmo},
	urldate = {2024-01-05},
	date = {2023-04-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2304.04625 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Han et al. - 2023 - Reinforcement Learning-Based Black-Box Model Inver.pdf:/Users/hannes/Zotero/storage/47VD5AQX/Han et al. - 2023 - Reinforcement Learning-Based Black-Box Model Inver.pdf:application/pdf},
}

@online{weber_hosthansba_code_2024,
	title = {hosthans/ba\_code},
	url = {https://github.com/hosthans/ba_code},
	abstract = {Code-Implementation for bachelor thesis.},
	titleaddon = {{GitHub}},
	author = {Weber, Hannes},
	urldate = {2024-01-05},
	date = {2024-01-06},
	langid = {english},
	file = {Snapshot:/Users/hannes/Zotero/storage/EG695LZ9/main.html:text/html},
}

@inproceedings{wen_defending_2021,
	location = {Rhodes, Greece},
	title = {Defending Against Model Inversion Attack by Adversarial Examples},
	isbn = {978-1-66540-285-9},
	url = {https://ieeexplore.ieee.org/document/9527945/},
	doi = {10.1109/CSR51186.2021.9527945},
	abstract = {Model inversion ({MI}) attacks aim to infer and reconstruct the input data from the output of a neural network, which poses a severe threat to the privacy of input data. Inspired by adversarial examples, we propose defending against {MI} attacks by adding adversarial noise to the output. The critical challenge is ﬁnding a noise vector that maximizes the inversion error and introduces negligible utility loss to the target model. We propose an algorithm to craft such noise vectors, which also incorporates utility-loss constraints. Speciﬁcally, our algorithm takes advantage of the gradient of an inversion model we train to mimic the adversary and compute a noise vector to turn the output into an adversarial example that can maximize the reconstruction error of the inversion model. Then we apply a label modiﬁer that keeps the label unchanged to achieve zero accuracy loss of the target model. Our defense does not tamper with the training process or need the private training dataset. Thus it can be easily applied to any current neural networks or {APIs}. We evaluate our method under both standard and adaptive attack settings. Our empirical results show our approach is effective against state-of-the-art {MI} attacks due to the transferability of adversarial examples and outperforms existing defenses. Furthermore, it causes more reconstruction errors while introducing zero accuracy loss and less distortion than existing defenses.},
	eventtitle = {2021 {IEEE} International Conference on Cyber Security and Resilience ({CSR})},
	pages = {551--556},
	booktitle = {2021 {IEEE} International Conference on Cyber Security and Resilience ({CSR})},
	publisher = {{IEEE}},
	author = {Wen, Jing and Yiu, Siu-Ming and Hui, Lucas C.K.},
	urldate = {2024-01-07},
	date = {2021-07-26},
	langid = {english},
	file = {Wen et al. - 2021 - Defending Against Model Inversion Attack by Advers.pdf:/Users/hannes/Zotero/storage/28TMZAYI/Wen et al. - 2021 - Defending Against Model Inversion Attack by Advers.pdf:application/pdf},
}

@misc{haarnoja_soft_2019,
	title = {Soft Actor-Critic Algorithms and Applications},
	url = {http://arxiv.org/abs/1812.05905},
	abstract = {Model-free deep reinforcement learning ({RL}) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic ({SAC}), our recently introduced off-policy actor-critic algorithm based on the maximum entropy {RL} framework. In this framework, the actor aims to simultaneously maximize expected return and entropy; that is, to succeed at the task while acting as randomly as possible. We extend {SAC} to incorporate a number of modiﬁcations that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate {SAC} on a range of benchmark tasks, as well as challenging real-world tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, {SAC} achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efﬁciency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that {SAC} is a promising candidate for learning in real-world robotics tasks.},
	number = {{arXiv}:1812.05905},
	publisher = {{arXiv}},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
	urldate = {2024-01-08},
	date = {2019-01-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1812.05905 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
	file = {Haarnoja et al. - 2019 - Soft Actor-Critic Algorithms and Applications.pdf:/Users/hannes/Zotero/storage/LSJZD3HY/Haarnoja et al. - 2019 - Soft Actor-Critic Algorithms and Applications.pdf:application/pdf},
}

@misc{yadav_cold_2019,
	title = {Cold Case: The Lost {MNIST} Digits},
	url = {http://arxiv.org/abs/1905.10498},
	shorttitle = {Cold Case},
	abstract = {Although the popular {MNIST} dataset [{LeCun} et al., 1994] is derived from the {NIST} database [Grother and Hanaoka, 1995], the precise processing steps for this derivation have been lost to time. We propose a reconstruction that is accurate enough to serve as a replacement for the {MNIST} dataset, with insigniﬁcant changes in accuracy. We trace each {MNIST} digit to its {NIST} source and its rich metadata such as writer identiﬁer, partition identiﬁer, etc. We also reconstruct the complete {MNIST} test set with 60,000 samples instead of the usual 10,000. Since the balance 50,000 were never distributed, they can be used to investigate the impact of twenty-ﬁve years of {MNIST} experiments on the reported testing performances. Our limited results unambiguously conﬁrm the trends observed by Recht et al. [2018, 2019]: although the misclassiﬁcation rates are slightly off, classiﬁer ordering and model selection remain broadly reliable. We attribute this phenomenon to the pairing beneﬁts of comparing classiﬁers on the same digits.},
	number = {{arXiv}:1905.10498},
	publisher = {{arXiv}},
	author = {Yadav, Chhavi and Bottou, Léon},
	urldate = {2024-01-09},
	date = {2019-11-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.10498 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Yadav und Bottou - 2019 - Cold Case The Lost MNIST Digits.pdf:/Users/hannes/Zotero/storage/QRL55WDQ/Yadav und Bottou - 2019 - Cold Case The Lost MNIST Digits.pdf:application/pdf},
}

@online{noauthor_gesetz_2021,
	title = {Das Gesetz {\textbar} {EU}-Gesetz über künstliche Intelligenz},
	url = {https://artificialintelligenceact.eu/de/das-gesetz/},
	urldate = {2024-01-11},
	date = {2021-04-21},
	langid = {german},
	file = {Snapshot:/Users/hannes/Zotero/storage/AC9MEGXF/das-gesetz.html:text/html},
}

@inproceedings{kanopoulos_design_1988,
	title = {Design of an image edge detection filter using the Sobel operator},
	volume = {23},
	url = {http://ieeexplore.ieee.org/document/996/},
	doi = {10.1109/4.996},
	pages = {358--367},
	booktitle = {{IEEE} Journal of Solid-State Circuits},
	author = {Kanopoulos, N. and Vasanthavada, N. and Baker, R.L.},
	urldate = {2024-01-17},
	date = {1988-04},
	langid = {english},
	file = {Kanopoulos et al. - 1988 - Design of an image edge detection filter using the.pdf:/Users/hannes/Zotero/storage/DJJCMNT2/Kanopoulos et al. - 1988 - Design of an image edge detection filter using the.pdf:application/pdf},
}

@inproceedings{machado_adversarial_2023,
	title = {Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective},
	volume = {55},
	url = {http://arxiv.org/abs/2009.03728},
	doi = {10.1145/3485133},
	shorttitle = {Adversarial Machine Learning in Image Classification},
	abstract = {Deep Learning algorithms have achieved the state-of-the-art performance for Image Classification and have been used even in security-critical applications, such as biometric recognition systems and self-driving cars. However, recent works have shown those algorithms, which can even surpass the human capabilities, are vulnerable to adversarial examples. In Computer Vision, adversarial examples are images containing subtle perturbations generated by malicious optimization algorithms in order to fool classifiers. As an attempt to mitigate these vulnerabilities, numerous countermeasures have been constantly proposed in literature. Nevertheless, devising an efficient defense mechanism has proven to be a difficult task, since many approaches have already shown to be ineffective to adaptive attackers. Thus, this self-containing paper aims to provide all readerships with a review of the latest research progress on Adversarial Machine Learning in Image Classification, however with a defender’s perspective. Here, novel taxonomies for categorizing adversarial attacks and defenses are introduced and discussions about the existence of adversarial examples are provided. Further, in contrast to exisiting surveys, it is also given relevant guidance that should be taken into consideration by researchers when devising and evaluating defenses. Finally, based on the reviewed literature, it is discussed some promising paths for future research. {CCS} Concepts: • Information systems → Decision support systems; • Security and privacy → Domainspecific security and privacy architectures; • Computing methodologies → Neural networks.},
	pages = {1--38},
	booktitle = {{ACM} Computing Surveys},
	author = {Machado, Gabriel Resende and Silva, Eugênio and Goldschmidt, Ronaldo Ribeiro},
	urldate = {2024-01-17},
	date = {2023-01-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2009.03728 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Machado et al. - 2023 - Adversarial Machine Learning in Image Classificati.pdf:/Users/hannes/Zotero/storage/7R83ARM9/Machado et al. - 2023 - Adversarial Machine Learning in Image Classificati.pdf:application/pdf},
}

@misc{ramirez_poisoning_2022,
	title = {Poisoning Attacks and Defenses on Artificial Intelligence: A Survey},
	url = {http://arxiv.org/abs/2202.10276},
	shorttitle = {Poisoning Attacks and Defenses on Artificial Intelligence},
	abstract = {Machine learning models have been widely adopted in several ﬁelds. However, most recent studies have shown several vulnerabilities from attacks with a potential to jeopardize the integrity of the model, presenting a new window of research opportunity in terms of cyber-security. This survey is conducted with a main intention of highlighting the most relevant information related to security vulnerabilities in the context of machine learning ({ML}) classiﬁers; more speciﬁcally, directed towards training procedures against data poisoning attacks, representing a type of attack that consists of tampering the data samples fed to the model during the training phase, leading to a degradation in the model’s overall accuracy during the inference phase. This work compiles the most relevant insights and ﬁndings found in the latest existing literatures addressing this type of attacks. Moreover, this paper also covers several defense techniques that promise feasible detection and mitigation mechanisms, capable of conferring a certain level of robustness to a target model against an attacker. A thorough assessment is performed on the reviewed works, comparing the effects of data poisoning on a wide range of {ML} models in real-world conditions, performing quantitative and qualitative analyses. This paper analyzes the main characteristics for each approach including performance success metrics, required hyperparameters, and deployment complexity. Moreover, this paper emphasizes the underlying assumptions and limitations considered by both attackers and defenders along with their intrinsic properties such as: availability, reliability, privacy, accountability, interpretability, etc. Finally, this paper concludes by making references of some of main existing research trends that provide pathways towards future research directions in the ﬁeld of cyber-security.},
	number = {{arXiv}:2202.10276},
	publisher = {{arXiv}},
	author = {Ramirez, Miguel A. and Kim, Song-Kyoo and Hamadi, Hussam Al and Damiani, Ernesto and Byon, Young-Ji and Kim, Tae-Yeon and Cho, Chung-Suk and Yeun, Chan Yeob},
	urldate = {2024-01-17},
	date = {2022-02-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2202.10276 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Ramirez et al. - 2022 - Poisoning Attacks and Defenses on Artificial Intel.pdf:/Users/hannes/Zotero/storage/DX455K3J/Ramirez et al. - 2022 - Poisoning Attacks and Defenses on Artificial Intel.pdf:application/pdf},
}

@online{noauthor_nvlabsstylegan_2024,
	title = {{NVlabs}/stylegan},
	url = {https://github.com/NVlabs/stylegan},
	abstract = {{StyleGAN} - Official {TensorFlow} Implementation},
	publisher = {{NVIDIA} Research Projects},
	urldate = {2024-01-17},
	date = {2024-01-17},
	note = {original-date: 2019-02-04T15:33:58Z},
}
