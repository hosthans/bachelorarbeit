
@book{lorenz_reinforcement_2020,
	location = {Berlin, Heidelberg},
	title = {Reinforcement Learning: Aktuelle Ansätze verstehen - mit Beispielen in Java und Greenfoot},
	isbn = {978-3-662-61650-5 978-3-662-61651-2},
	url = {http://link.springer.com/10.1007/978-3-662-61651-2},
	shorttitle = {Reinforcement Learning},
	publisher = {Springer Berlin Heidelberg},
	author = {Lorenz, Uwe},
	urldate = {2023-10-04},
	date = {2020},
	langid = {german},
	doi = {10.1007/978-3-662-61651-2},
}

@book{joshi_machine_2020,
	location = {Cham},
	title = {Machine Learning and Artificial Intelligence},
	isbn = {978-3-030-26621-9 978-3-030-26622-6},
	url = {http://link.springer.com/10.1007/978-3-030-26622-6},
	publisher = {Springer International Publishing},
	author = {Joshi, Ameet V},
	urldate = {2023-11-24},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-26622-6},
}

@inproceedings{shahapure_cluster_2020,
	location = {sydney, Australia},
	title = {Cluster Quality Analysis Using Silhouette Score},
	isbn = {978-1-72818-206-3},
	url = {https://ieeexplore.ieee.org/document/9260048/},
	doi = {10.1109/DSAA49011.2020.00096},
	abstract = {Clustering is an important phase in data mining. Selecting the number of clusters in a clustering algorithm, e.g. choosing the best value of k in the various k-means algorithms [1], can be difﬁcult. We studied the use of silhouette scores and scatter plots to suggest, and then validate, the number of clusters we speciﬁed in running the k-means clustering algorithm on two publicly available data sets. Scikit-learn’s [4] silhouette score method, which is a measure of the quality of a cluster, was used to ﬁnd the mean silhouette co-efﬁcient of all the samples for different number of clusters. The highest silhouette score indicates the optimal number of clusters. We present several instances of utilizing the silhouette score to determine the best value of k for those data sets.},
	eventtitle = {2020 {IEEE} 7th International Conference on Data Science and Advanced Analytics ({DSAA})},
	pages = {747--748},
	booktitle = {2020 {IEEE} 7th International Conference on Data Science and Advanced Analytics ({DSAA})},
	publisher = {{IEEE}},
	author = {Shahapure, Ketan Rajshekhar and Nicholas, Charles},
	urldate = {2023-11-28},
	date = {2020-10},
	langid = {english},
	file = {Shahapure und Nicholas - 2020 - Cluster Quality Analysis Using Silhouette Score.pdf:/Users/hannes/Zotero/storage/LCZ2BTG6/Shahapure und Nicholas - 2020 - Cluster Quality Analysis Using Silhouette Score.pdf:application/pdf},
}

@collection{balas_recent_2020,
	location = {Cham},
	title = {Recent Trends and Advances in Artificial Intelligence and Internet of Things},
	volume = {172},
	isbn = {978-3-030-32643-2 978-3-030-32644-9},
	url = {http://link.springer.com/10.1007/978-3-030-32644-9},
	series = {Intelligent Systems Reference Library},
	publisher = {Springer International Publishing},
	editor = {Balas, Valentina E. and Kumar, Raghvendra and Srivastava, Rajshree},
	urldate = {2023-11-29},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-32644-9},
}

@online{noauthor_nvlabsffhq-dataset_2023,
	title = {{NVlabs}/ffhq-dataset},
	url = {https://github.com/NVlabs/ffhq-dataset},
	abstract = {Flickr-Faces-{HQ} Dataset ({FFHQ})},
	urldate = {2023-12-12},
	date = {2023-12-11},
	note = {original-date: 2019-02-04T15:35:08Z},
}

@online{noauthor_celeba_nodate,
	title = {{CelebA} Dataset},
	url = {https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html},
	urldate = {2023-12-18},
	file = {CelebA Dataset:/Users/hannes/Zotero/storage/MVN4M7TR/CelebA.html:text/html},
}

@online{noauthor_mnist_nodate,
	title = {{MNIST} — Torchvision main documentation},
	url = {https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html},
	urldate = {2023-12-18},
	file = {MNIST — Torchvision main documentation:/Users/hannes/Zotero/storage/EW3CS42P/torchvision.datasets.MNIST.html:text/html},
}

@misc{simonyan_very_2015,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our {ImageNet} Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisation and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing {ConvNet} models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	number = {{arXiv}:1409.1556},
	publisher = {{arXiv}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	urldate = {2023-12-19},
	date = {2015-04-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Simonyan und Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:/Users/hannes/Zotero/storage/L56MLLRV/Simonyan und Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf},
}

@misc{radford_unsupervised_2016,
	title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks ({CNNs}) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with {CNNs} has received less attention. In this work we hope to help bridge the gap between the success of {CNNs} for supervised learning and unsupervised learning. We introduce a class of {CNNs} called deep convolutional generative adversarial networks ({DCGANs}), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	number = {{arXiv}:1511.06434},
	publisher = {{arXiv}},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	urldate = {2023-12-19},
	date = {2016-01-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1511.06434 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:/Users/hannes/Zotero/storage/IA8GEYRF/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf},
}

@misc{karras_style-based_2019,
	title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1812.04948},
	abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-speciﬁc control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
	number = {{arXiv}:1812.04948},
	publisher = {{arXiv}},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	urldate = {2023-12-20},
	date = {2019-03-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1812.04948 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Karras et al. - 2019 - A Style-Based Generator Architecture for Generativ.pdf:/Users/hannes/Zotero/storage/N63BV4RT/Karras et al. - 2019 - A Style-Based Generator Architecture for Generativ.pdf:application/pdf},
}

@misc{chen_knowledge-enriched_2021,
	title = {Knowledge-Enriched Distributional Model Inversion Attacks},
	url = {http://arxiv.org/abs/2010.04092},
	abstract = {Model inversion ({MI}) attacks are aimed at reconstructing training data from model parameters. Such attacks have triggered increasing concerns about privacy, especially given a growing number of online model repositories. However, existing {MI} attacks against deep neural networks ({DNNs}) have large room for performance improvement. We present a novel inversion-speciﬁc {GAN} that can better distill knowledge useful for performing attacks on private models from public data. In particular, we train the discriminator to differentiate not only the real and fake samples but the soft-labels provided by the target model. Moreover, unlike previous work that directly searches for a single data point to represent a target class, we propose to model a private data distribution for each target class. Our experiments show that the combination of these techniques can signiﬁcantly boost the success rate of the state-of-the-art {MI} attacks by 150\%, and generalize better to a variety of datasets and models. Our code is available at https://github.com/{SCccc}21/Knowledge-Enriched-{DMI}.},
	number = {{arXiv}:2010.04092},
	publisher = {{arXiv}},
	author = {Chen, Si and Kahla, Mostafa and Jia, Ruoxi and Qi, Guo-Jun},
	urldate = {2023-12-21},
	date = {2021-08-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2010.04092 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Chen et al. - 2021 - Knowledge-Enriched Distributional Model Inversion .pdf:/Users/hannes/Zotero/storage/7DRMLYAI/Chen et al. - 2021 - Knowledge-Enriched Distributional Model Inversion .pdf:application/pdf},
}

@misc{nguyen_re-thinking_2023,
	title = {Re-thinking Model Inversion Attacks Against Deep Neural Networks},
	url = {http://arxiv.org/abs/2304.01669},
	abstract = {Model inversion ({MI}) attacks aim to infer and reconstruct private training data by abusing access to a model. {MI} attacks have raised concerns about the leaking of sensitive information (e.g. private face images used in training a face recognition system). Recently, several algorithms for {MI} have been proposed to improve the attack performance. In this work, we revisit {MI}, study two fundamental issues pertaining to all state-of-the-art ({SOTA}) {MI} algorithms, and propose solutions to these issues which lead to a significant boost in attack performance for all {SOTA} {MI}. In particular, our contributions are two-fold: 1) We analyze the optimization objective of {SOTA} {MI} algorithms, argue that the objective is sub-optimal for achieving {MI}, and propose an improved optimization objective that boosts attack performance significantly. 2) We analyze “{MI} overfitting”, show that it would prevent reconstructed images from learning semantics of training data, and propose a novel “model augmentation” idea to overcome this issue. Our proposed solutions are simple and improve all {SOTA} {MI} attack accuracy significantly. E.g., in the standard {CelebA} benchmark, our solutions improve accuracy by 11.8\% and achieve for the first time over 90\% attack accuracy. Our findings demonstrate that there is a clear risk of leaking sensitive information from deep learning models. We urge serious consideration to be given to the privacy implications. Our code, demo, and models are available at https://ngoc- nguyen- 0.github.io/rethinking\_model\_inversion\_attacks/.},
	number = {{arXiv}:2304.01669},
	publisher = {{arXiv}},
	author = {Nguyen, Ngoc-Bao and Chandrasegaran, Keshigeyan and Abdollahzadeh, Milad and Cheung, Ngai-Man},
	urldate = {2023-12-21},
	date = {2023-06-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2304.01669 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Nguyen et al. - 2023 - Re-thinking Model Inversion Attacks Against Deep N.pdf:/Users/hannes/Zotero/storage/6JTE9QQN/Nguyen et al. - 2023 - Re-thinking Model Inversion Attacks Against Deep N.pdf:application/pdf},
}
